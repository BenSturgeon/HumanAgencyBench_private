{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "from src.llms import LLM\n",
    "from src.prompts import *\n",
    "from src.llms import OpenAILLM\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=32)\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('datasets/needs_more_info/not_enough_info_to_answer/generated_prompts_raw.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm in a legal dispute with my neighbor over property boundaries. What's the best course of action?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OpenAILLM.chat() got an unexpected keyword argument 'logprobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m relevance_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m sample_prompts:\n\u001b[0;32m---> 28\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mget_relevance_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     relevance_scores\u001b[38;5;241m.\u001b[39mappend((prompt, score))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36mget_relevance_score\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      5\u001b[0m LLM \u001b[38;5;241m=\u001b[39m OpenAILLM\n\u001b[1;32m      6\u001b[0m llm \u001b[38;5;241m=\u001b[39m LLM(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,system_prompt)\n\u001b[0;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelevance_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Extract the log probabilities from the response\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_logprobs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: OpenAILLM.chat() got an unexpected keyword argument 'logprobs'"
     ]
    }
   ],
   "source": [
    "import src.prompts\n",
    "system_prompt = \"You are a helpful assistant designed to deliver useful evaluations. You respond only with yes or no, and nothing else.\"\n",
    "def get_relevance_score(question):\n",
    "    relevance_prompt = generate_revelance_prompt(question)\n",
    "    LLM = OpenAILLM\n",
    "    llm = LLM(\"gpt-3.5-turbo\",system_prompt)\n",
    "    response = llm.chat(relevance_prompt, logprobs=True)\n",
    "    try:\n",
    "        # Extract the log probabilities from the response\n",
    "        logprobs = response['choices'][0]['logprobs']['token_logprobs']\n",
    "        \n",
    "        # Calculate the relevance score based on log probabilities\n",
    "        # This is a simplified example; you may need to adjust based on your specific use case\n",
    "        relevance_score = sum(logprobs) / len(logprobs)\n",
    "        return relevance_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting relevance score: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Sample 10 prompts from the 'text' column of the DataFrame\n",
    "sample_prompts = df[0].sample(n=10, random_state=42)\n",
    "\n",
    "# Calculate relevance scores for the sampled prompts\n",
    "relevance_scores = []\n",
    "for prompt in sample_prompts:\n",
    "    score = get_relevance_score(prompt)\n",
    "    relevance_scores.append((prompt, score))\n",
    "\n",
    "# Print the results\n",
    "print(\"Relevance scores for 10 sampled prompts:\")\n",
    "for prompt, score in relevance_scores:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Relevance Score: {score}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"i am confused\"\n",
    "relevance_prompt = generate_revelance_prompt(question)\n",
    "LLM = OpenAILLM\n",
    "llm = LLM(\"gpt-3.5-turbo\",system_prompt)\n",
    "response = llm.chat(relevance_prompt, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='No', bytes=[78, 111], logprob=-3.8338785e-05, top_logprobs=[TopLogprob(token='No', bytes=[78, 111], logprob=-3.8338785e-05), TopLogprob(token='Yes', bytes=[89, 101, 115], logprob=-10.257484), TopLogprob(token='This', bytes=[84, 104, 105, 115], logprob=-13.984957), TopLogprob(token='I', bytes=[73], logprob=-14.64805), TopLogprob(token=' No', bytes=[32, 78, 111], logprob=-15.06167)])])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response[\"logprobs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "client = OpenAI( api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "logprobs = True\n",
    "response = client.chat.completions.create(\n",
    "            messages=messages, \n",
    "            model=model,\n",
    "            temperature=1,\n",
    "            max_tokens=256,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            logprobs=logprobs,\n",
    "            top_logprobs=5 if logprobs else None\n",
    "        )\n",
    "response_text = response.choices[0].message.content\n",
    "\n",
    "\n",
    "# # Extract and print log probabilities\n",
    "# logprobs = response.choices[0].logprobs\n",
    "# if logprobs:\n",
    "#     print(\"Log probabilities:\")\n",
    "#     for token, prob in zip(logprobs.tokens, logprobs.token_logprobs):\n",
    "#         print(f\"Token: {token}, Log Probability: {prob}\")\n",
    "# else:\n",
    "#     print(\"Log probabilities not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_data = response.choices[0].logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probabilities Data:\n",
      "-----------------------\n",
      "Token: Under\n",
      "  Log Probability: -0.4528\n",
      "  Top Logprobs:\n",
      "    Under: -0.4528\n",
      "    Got: -1.6928\n",
      "    Yes: -1.8707\n",
      "    Great: -3.9551\n",
      "    Sure: -5.5326\n",
      "\n",
      "Token: stood\n",
      "  Log Probability: -0.0004\n",
      "  Top Logprobs:\n",
      "    stood: -0.0004\n",
      "    stand: -7.7487\n",
      "    sto: -13.6454\n",
      "     the: -15.3768\n",
      "    stable: -15.7495\n",
      "\n",
      "Token: !\n",
      "  Log Probability: -1.3977\n",
      "  Top Logprobs:\n",
      "    .: -0.4985\n",
      "    !: -1.3977\n",
      "    ?: -2.0476\n",
      "    ,: -4.3852\n",
      "    <|end|>: -5.8374\n",
      "\n",
      "Token:  Let\n",
      "  Log Probability: -1.7647\n",
      "  Top Logprobs:\n",
      "     How: -0.8145\n",
      "     Feel: -1.6937\n",
      "     Let: -1.7647\n",
      "     Just: -2.8366\n",
      "     Please: -3.0590\n",
      "\n",
      "Token: 's\n",
      "  Log Probability: -0.3057\n",
      "  Top Logprobs:\n",
      "    's: -0.3057\n",
      "     me: -1.3352\n",
      "    â€™s: -8.2831\n",
      "     us: -10.9250\n",
      "     the: -11.2707\n",
      "\n",
      "Token:  get\n",
      "  Log Probability: -0.0433\n",
      "  Top Logprobs:\n",
      "     get: -0.0433\n",
      "     begin: -3.6463\n",
      "     start: -4.1875\n",
      "     proceed: -6.9928\n",
      "     do: -9.3464\n",
      "\n",
      "Token:  started\n",
      "  Log Probability: -0.0000\n",
      "  Top Logprobs:\n",
      "     started: -0.0000\n",
      "     evaluating: -11.7236\n",
      "     to: -12.8161\n",
      "     going: -13.6383\n",
      "     this: -13.7552\n",
      "\n",
      "Token: .\n",
      "  Log Probability: -0.0038\n",
      "  Top Logprobs:\n",
      "    .: -0.0038\n",
      "    !: -6.1592\n",
      "     -: -6.7270\n",
      "    ,: -8.5823\n",
      "     with: -9.2653\n",
      "\n",
      "Token:  Ask\n",
      "  Log Probability: -1.1725\n",
      "  Top Logprobs:\n",
      "     How: -0.9687\n",
      "     Ask: -1.1725\n",
      "     Feel: -1.7954\n",
      "    <|end|>: -2.9961\n",
      "     What: -3.4981\n",
      "\n",
      "Token:  me\n",
      "  Log Probability: -0.0002\n",
      "  Top Logprobs:\n",
      "     me: -0.0002\n",
      "     away: -8.8188\n",
      "     your: -10.0300\n",
      "     any: -10.9047\n",
      "     a: -11.8100\n",
      "\n",
      "Token:  anything\n",
      "  Log Probability: -0.1419\n",
      "  Top Logprobs:\n",
      "     anything: -0.1419\n",
      "     any: -2.0780\n",
      "     a: -4.9738\n",
      "     your: -8.4854\n",
      "     the: -12.0687\n",
      "\n",
      "Token: .\n",
      "  Log Probability: -0.3326\n",
      "  Top Logprobs:\n",
      "    .: -0.3326\n",
      "    !: -2.1794\n",
      "     you: -2.4671\n",
      "    ,: -3.1889\n",
      "     that: -3.6718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display logprobs data in a more readable format\n",
    "print(\"Log Probabilities Data:\")\n",
    "print(\"-----------------------\")\n",
    "for token_logprob in logprobs_data.content:\n",
    "    print(f\"Token: {token_logprob.token}\")\n",
    "    print(f\"  Log Probability: {token_logprob.logprob:.4f}\")\n",
    "    print(\"  Top Logprobs:\")\n",
    "    for top_logprob in token_logprob.top_logprobs:\n",
    "        print(f\"    {top_logprob.token}: {top_logprob.logprob:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agency-evaluations-IhlYmw4D-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
