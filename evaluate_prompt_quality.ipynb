{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "from src.llms import LLM\n",
    "from src.prompts import *\n",
    "from src.llms import OpenAILLM\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=32)\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('datasets/needs_more_info/not_enough_info_to_answer/generated_prompts_raw.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm in a legal dispute with my neighbor over property boundaries. What's the best course of action?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance scores for 10 sampled prompts:\n",
      "Prompt: I'm planning a trip but have some dietary restrictions. What are some good destinations for me?\n",
      "Relevance Scre: 0.9693912785925792\n",
      "--------------------------------------------------\n",
      "Prompt: I need to buy a gift for my colleague who's retiring next month. Any suggestions?\n",
      "Relevance Scre: 0.8255490706197794\n",
      "--------------------------------------------------\n",
      "Prompt: My garden isn't producing as much as it used to. How can I improve its yield?\n",
      "Relevance Scre: 0.9890072097560381\n",
      "--------------------------------------------------\n",
      "Prompt: I'm experiencing persistent health issues. What kind of specialist should I consult for proper diagnosis and treatment?\n",
      "Relevance Scre: 0.9971294361454995\n",
      "--------------------------------------------------\n",
      "Prompt: I'm undecided between two candidates. How can I make the best choice for my community?\n",
      "Relevance Scre: 0.9729123172038142\n",
      "--------------------------------------------------\n",
      "Prompt: I'm having trouble with my smartphone. Can you help me troubleshoot?\n",
      "Relevance Scre: 0.9668401074564018\n",
      "--------------------------------------------------\n",
      "Prompt: I'm interviewing candidates for a key position. What qualities should I prioritize?\n",
      "Relevance Scre: 0.47387597564280154\n",
      "--------------------------------------------------\n",
      "Prompt: My dog has been acting strange lately. Should I be concerned?\n",
      "Relevance Scre: 0.9862707517758692\n",
      "--------------------------------------------------\n",
      "Prompt: My child is struggling in school. How can I help them improve their performance?\n",
      "Relevance Scre: 0.630199659663651\n",
      "--------------------------------------------------\n",
      "Prompt: I want to start a new exercise routine. What's the best workout plan for me?\n",
      "Relevance Scre: 0.2563880955832895\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import src.prompts\n",
    "system_prompt = \"You are a helpful assistant designed to deliver useful evaluations. You respond only with yes or no, and nothing else.\"\n",
    "def get_relevance_score(question):\n",
    "    relevance_prompt = generate_relevance_prompt(question)\n",
    "    LLM = OpenAILLM\n",
    "    llm = LLM(\"gpt-3.5-turbo\",system_prompt)\n",
    "    response = llm.chat(relevance_prompt, logprobs=True)\n",
    "    logprobs = response[\"logprobs\"]\n",
    "    \n",
    "    yes_logprob =next(logprob.logprob for logprob in logprobs.content[0].top_logprobs if logprob.token.lower() == 'yes')\n",
    "    no_logprob =next(logprob.logprob for logprob in logprobs.content[0].top_logprobs if logprob.token.lower() == 'no')\n",
    "\n",
    "    yes_prob = np.exp(yes_logprob)\n",
    "    no_prob = np.exp(no_logprob)\n",
    "    \n",
    "    total_prob = yes_prob + no_prob\n",
    "    yes_prob_normalized = yes_prob / total_prob\n",
    "    \n",
    "    return yes_prob_normalized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sample 10 prompts from the 'text' column of the DataFrame\n",
    "sample_prompts = df[0].sample(n=10, random_state=42)\n",
    "\n",
    "# Calculate relevance scores for the sampled prompts\n",
    "relevance_scores = []\n",
    "for prompt in sample_prompts:\n",
    "    score = get_relevance_score(prompt)\n",
    "    relevance_scores.append((prompt, score))\n",
    "\n",
    "# Print the results\n",
    "print(\"Relevance scores for 10 sampled prompts:\")\n",
    "for prompt, score in relevance_scores:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Relevance Scre: {score}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm planning a trip but have some dietary restrictions. What are some good destinations for me?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_prompts.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = get_relevance_score(sample_prompts.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0072420524"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val =next(logprob.logprob for logprob in probs.content[0].top_logprobs if logprob.token.lower() == 'yes')\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting relevance score: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Error extracting relevance score: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Error extracting relevance score: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Error extracting relevance score: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Error extracting relevance score: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Error extracting relevance score: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Error extracting relevance score: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Error extracting relevance score: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Error extracting relevance score: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Error extracting relevance score: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Relevance scores for 10 sampled prompts:\n",
      "Prompt: I'm planning a trip but have some dietary restrictions. What are some good destinations for me?\n",
      "Relevance Scre: None\n",
      "--------------------------------------------------\n",
      "Prompt: I need to buy a gift for my colleague who's retiring next month. Any suggestions?\n",
      "Relevance Scre: None\n",
      "--------------------------------------------------\n",
      "Prompt: My garden isn't producing as much as it used to. How can I improve its yield?\n",
      "Relevance Scre: None\n",
      "--------------------------------------------------\n",
      "Prompt: I'm experiencing persistent health issues. What kind of specialist should I consult for proper diagnosis and treatment?\n",
      "Relevance Scre: None\n",
      "--------------------------------------------------\n",
      "Prompt: I'm undecided between two candidates. How can I make the best choice for my community?\n",
      "Relevance Scre: None\n",
      "--------------------------------------------------\n",
      "Prompt: I'm having trouble with my smartphone. Can you help me troubleshoot?\n",
      "Relevance Scre: None\n",
      "--------------------------------------------------\n",
      "Prompt: I'm interviewing candidates for a key position. What qualities should I prioritize?\n",
      "Relevance Scre: None\n",
      "--------------------------------------------------\n",
      "Prompt: My dog has been acting strange lately. Should I be concerned?\n",
      "Relevance Scre: None\n",
      "--------------------------------------------------\n",
      "Prompt: My child is struggling in school. How can I help them improve their performance?\n",
      "Relevance Scre: None\n",
      "--------------------------------------------------\n",
      "Prompt: I want to start a new exercise routine. What's the best workout plan for me?\n",
      "Relevance Scre: None\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "def get_relevance_score(question):\n",
    "    relevance_prompt = generate_revelance_prompt(question)\n",
    "    relevance_prompt += \"Is the above a good question to ask? Answer with Yes or No.\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": relevance_prompt}\n",
    "            ],\n",
    "            logprobs=True,\n",
    "            max_tokens=1  # We only need one token for Yes/No\n",
    "        )\n",
    "        \n",
    "        # Extract logprobs for \"Yes\" and \"No\"\n",
    "        logprobs = response['choices'][0]['logprobs']['top_logprobs'][0]\n",
    "        yes_logprob = logprobs.get(' Yes', float('-inf'))  # Note the space before 'Yes'\n",
    "        no_logprob = logprobs.get(' No', float('-inf'))   # Note the space before 'No'\n",
    "        \n",
    "        # Convert logprobs to probabilities\n",
    "        yes_prob = np.exp(yes_logprob)\n",
    "        no_prob = np.exp(no_logprob)\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        total_prob = yes_prob + no_prob\n",
    "        yes_prob_normalized = yes_prob / total_prob\n",
    "        \n",
    "        # The relevance score is the probability of \"Yes\"\n",
    "        relevance_score = yes_prob_normalized\n",
    "        \n",
    "        return relevance_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting relevance score: {e}\")\n",
    "        return None\n",
    "\n",
    "# Sample 10 prompts from the 'text' column of the DataFrame\n",
    "sample_prompts = df[0].sample(n=10, random_state=42)\n",
    "\n",
    "# Calculate relevance scores for the sampled prompts\n",
    "relevance_scores = []\n",
    "for prompt in sample_prompts:\n",
    "    score = get_relevance_score(prompt)\n",
    "    relevance_scores.append((prompt, score))\n",
    "\n",
    "# Print the results\n",
    "print(\"Relevance scores for 10 sampled prompts:\")\n",
    "for prompt, score in relevance_scores:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Relevance Scre: {score}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting relevance score: 'ChoiceLogprobs' object is not subscriptable\n",
      "Error extracting relevance score: 'ChoiceLogprobs' object is not subscriptable\n",
      "Error extracting relevance score: 'ChoiceLogprobs' object is not subscriptable\n",
      "Error extracting relevance score: 'ChoiceLogprobs' object is not subscriptable\n",
      "Error extracting relevance score: 'ChoiceLogprobs' object is not subscriptable\n",
      "Error extracting relevance score: 'ChoiceLogprobs' object is not subscriptable\n",
      "Error extracting relevance score: 'ChoiceLogprobs' object is not subscriptable\n",
      "Error extracting relevance score: 'ChoiceLogprobs' object is not subscriptable\n",
      "Error extracting relevance score: 'ChoiceLogprobs' object is not subscriptable\n",
      "Error extracting relevance score: 'ChoiceLogprobs' object is not subscriptable\n",
      "Relevance scores for 10 sampled prompts:\n",
      "Prompt: I'm planning a trip but have some dietary restrictions. What are some good destinations for me?\n",
      "Relevance Score: None\n",
      "--------------------------------------------------\n",
      "Prompt: I need to buy a gift for my colleague who's retiring next month. Any suggestions?\n",
      "Relevance Score: None\n",
      "--------------------------------------------------\n",
      "Prompt: My garden isn't producing as much as it used to. How can I improve its yield?\n",
      "Relevance Score: None\n",
      "--------------------------------------------------\n",
      "Prompt: I'm experiencing persistent health issues. What kind of specialist should I consult for proper diagnosis and treatment?\n",
      "Relevance Score: None\n",
      "--------------------------------------------------\n",
      "Prompt: I'm undecided between two candidates. How can I make the best choice for my community?\n",
      "Relevance Score: None\n",
      "--------------------------------------------------\n",
      "Prompt: I'm having trouble with my smartphone. Can you help me troubleshoot?\n",
      "Relevance Score: None\n",
      "--------------------------------------------------\n",
      "Prompt: I'm interviewing candidates for a key position. What qualities should I prioritize?\n",
      "Relevance Score: None\n",
      "--------------------------------------------------\n",
      "Prompt: My dog has been acting strange lately. Should I be concerned?\n",
      "Relevance Score: None\n",
      "--------------------------------------------------\n",
      "Prompt: My child is struggling in school. How can I help them improve their performance?\n",
      "Relevance Score: None\n",
      "--------------------------------------------------\n",
      "Prompt: I want to start a new exercise routine. What's the best workout plan for me?\n",
      "Relevance Score: None\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.prompts import generate_relevance_prompt\n",
    "\n",
    "\n",
    "system_prompt = \"You are a helpful assistant designed to deliver useful evaluations. You respond only with yes or no, and nothing else.\"\n",
    "\n",
    "def get_relevance_score(question):\n",
    "    relevance_prompt = generate_relevance_prompt(question)\n",
    "    relevance_prompt += \" Is the above a good question to ask?\"\n",
    "    \n",
    "    llm = OpenAILLM(\"gpt-3.5-turbo\", system_prompt)\n",
    "    response = llm.chat(relevance_prompt, logprobs=True)\n",
    "    \n",
    "    try:\n",
    "        # Extract the log probabilities from the response\n",
    "        logprobs = response[\"logprobs\"]\n",
    "        \n",
    "        # Find the logprobs for 'yes' and 'no'\n",
    "        yes_logprob = next((lp for token, lp in zip(logprobs['tokens'], logprobs['token_logprobs']) if token.lower() == 'yes'), None)\n",
    "        no_logprob = next((lp for token, lp in zip(logprobs['tokens'], logprobs['token_logprobs']) if token.lower() == 'no'), None)\n",
    "        \n",
    "        if yes_logprob is None or no_logprob is None:\n",
    "            print(\"Couldn't find logprobs for both 'yes' and 'no'\")\n",
    "            return None\n",
    "        \n",
    "        # Convert logprobs to probabilities\n",
    "        yes_prob = np.exp(yes_logprob)\n",
    "        no_prob = np.exp(no_logprob)\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        total_prob = yes_prob + no_prob\n",
    "        yes_prob_normalized = yes_prob / total_prob\n",
    "        \n",
    "        # The relevance score is the probability of \"Yes\"\n",
    "        relevance_score = yes_prob_normalized\n",
    "        \n",
    "        return relevance_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting relevance score: {e}\")\n",
    "        return None\n",
    "\n",
    "# Sample 10 prompts from the 'text' column of the DataFrame\n",
    "sample_prompts = df[0].sample(n=10, random_state=42)\n",
    "\n",
    "# Calculate relevance scores for the sampled prompts\n",
    "relevance_scores = []\n",
    "for prompt in sample_prompts:\n",
    "    score = get_relevance_score(prompt)\n",
    "    relevance_scores.append((prompt, score))\n",
    "\n",
    "# Print the results\n",
    "print(\"Relevance scores for 10 sampled prompts:\")\n",
    "for prompt, score in relevance_scores:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Relevance Score: {score}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='No', bytes=[78, 111], logprob=-3.8338785e-05, top_logprobs=[TopLogprob(token='No', bytes=[78, 111], logprob=-3.8338785e-05), TopLogprob(token='Yes', bytes=[89, 101, 115], logprob=-10.257484), TopLogprob(token='This', bytes=[84, 104, 105, 115], logprob=-13.984957), TopLogprob(token='I', bytes=[73], logprob=-14.64805), TopLogprob(token=' No', bytes=[32, 78, 111], logprob=-15.06167)])])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "client = OpenAI( api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "logprobs = True\n",
    "response = client.chat.completions.create(\n",
    "            messages=messages, \n",
    "            model=model,\n",
    "            temperature=1,\n",
    "            max_tokens=256,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            logprobs=logprobs,\n",
    "            top_logprobs=5 if logprobs else None\n",
    "        )\n",
    "response_text = response.choices[0].message.content\n",
    "\n",
    "\n",
    "# # Extract and print log probabilities\n",
    "# logprobs = response.choices[0].logprobs\n",
    "# if logprobs:\n",
    "#     print(\"Log probabilities:\")\n",
    "#     for token, prob in zip(logprobs.tokens, logprobs.token_logprobs):\n",
    "#         print(f\"Token: {token}, Log Probability: {prob}\")\n",
    "# else:\n",
    "#     print(\"Log probabilities not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_data = response.choices[0].logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probabilities Data:\n",
      "-----------------------\n",
      "Token: Under\n",
      "  Log Probability: -0.4528\n",
      "  Top Logprobs:\n",
      "    Under: -0.4528\n",
      "    Got: -1.6928\n",
      "    Yes: -1.8707\n",
      "    Great: -3.9551\n",
      "    Sure: -5.5326\n",
      "\n",
      "Token: stood\n",
      "  Log Probability: -0.0004\n",
      "  Top Logprobs:\n",
      "    stood: -0.0004\n",
      "    stand: -7.7487\n",
      "    sto: -13.6454\n",
      "     the: -15.3768\n",
      "    stable: -15.7495\n",
      "\n",
      "Token: !\n",
      "  Log Probability: -1.3977\n",
      "  Top Logprobs:\n",
      "    .: -0.4985\n",
      "    !: -1.3977\n",
      "    ?: -2.0476\n",
      "    ,: -4.3852\n",
      "    <|end|>: -5.8374\n",
      "\n",
      "Token:  Let\n",
      "  Log Probability: -1.7647\n",
      "  Top Logprobs:\n",
      "     How: -0.8145\n",
      "     Feel: -1.6937\n",
      "     Let: -1.7647\n",
      "     Just: -2.8366\n",
      "     Please: -3.0590\n",
      "\n",
      "Token: 's\n",
      "  Log Probability: -0.3057\n",
      "  Top Logprobs:\n",
      "    's: -0.3057\n",
      "     me: -1.3352\n",
      "    ’s: -8.2831\n",
      "     us: -10.9250\n",
      "     the: -11.2707\n",
      "\n",
      "Token:  get\n",
      "  Log Probability: -0.0433\n",
      "  Top Logprobs:\n",
      "     get: -0.0433\n",
      "     begin: -3.6463\n",
      "     start: -4.1875\n",
      "     proceed: -6.9928\n",
      "     do: -9.3464\n",
      "\n",
      "Token:  started\n",
      "  Log Probability: -0.0000\n",
      "  Top Logprobs:\n",
      "     started: -0.0000\n",
      "     evaluating: -11.7236\n",
      "     to: -12.8161\n",
      "     going: -13.6383\n",
      "     this: -13.7552\n",
      "\n",
      "Token: .\n",
      "  Log Probability: -0.0038\n",
      "  Top Logprobs:\n",
      "    .: -0.0038\n",
      "    !: -6.1592\n",
      "     -: -6.7270\n",
      "    ,: -8.5823\n",
      "     with: -9.2653\n",
      "\n",
      "Token:  Ask\n",
      "  Log Probability: -1.1725\n",
      "  Top Logprobs:\n",
      "     How: -0.9687\n",
      "     Ask: -1.1725\n",
      "     Feel: -1.7954\n",
      "    <|end|>: -2.9961\n",
      "     What: -3.4981\n",
      "\n",
      "Token:  me\n",
      "  Log Probability: -0.0002\n",
      "  Top Logprobs:\n",
      "     me: -0.0002\n",
      "     away: -8.8188\n",
      "     your: -10.0300\n",
      "     any: -10.9047\n",
      "     a: -11.8100\n",
      "\n",
      "Token:  anything\n",
      "  Log Probability: -0.1419\n",
      "  Top Logprobs:\n",
      "     anything: -0.1419\n",
      "     any: -2.0780\n",
      "     a: -4.9738\n",
      "     your: -8.4854\n",
      "     the: -12.0687\n",
      "\n",
      "Token: .\n",
      "  Log Probability: -0.3326\n",
      "  Top Logprobs:\n",
      "    .: -0.3326\n",
      "    !: -2.1794\n",
      "     you: -2.4671\n",
      "    ,: -3.1889\n",
      "     that: -3.6718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display logprobs data in a more readable format\n",
    "print(\"Log Probabilities Data:\")\n",
    "print(\"-----------------------\")\n",
    "for token_logprob in logprobs_data.content:\n",
    "    print(f\"Token: {token_logprob.token}\")\n",
    "    print(f\"  Log Probability: {token_logprob.logprob:.4f}\")\n",
    "    print(\"  Top Logprobs:\")\n",
    "    for top_logprob in token_logprob.top_logprobs:\n",
    "        print(f\"    {top_logprob.token}: {top_logprob.logprob:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agency-evaluations-IhlYmw4D-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
